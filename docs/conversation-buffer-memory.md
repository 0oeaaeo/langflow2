`ConversationBufferMemory` allows you to store messages and then extract the messages in a variable. The **memory key** input is typically generated by encoding the input text using an encoder network, which maps the input text into a fixed-dimensional vector representation.

<br>

![Description](img/single_node/conv_buf_mem.png#only-light){width=50%}
![Description](img/single_node/conv_buf_mem2.png#only-dark){width=50%}

<br>

Learn more about the [ConversationBufferMemory](https://python.langchain.com/en/latest/modules/memory/types/buffer.html){.internal-link target=\_blank} in the LangChain documentation.

---
### ⛓️LangFlow example

<!-- <figure markdown> -->

![Description](img/conversation-buffer-memory2.png#only-dark){width=100%}
![Description](img/conversation-buffer-memory.png#only-light){width=100%}

<br>

[Download Flow](data/Conversation_buffer_memory.json){: .md-button download="Conversation_buffer_memory"}

<br>

`ConversationChain` is a chain to have a conversation and load context from memory. **Output Key** and **Input Key** are simply unique identifiers used to represent the data being passed between different modules or steps in a Conversation Chain. These keys help to ensure that the data is properly routed and processed by the appropriate modules in the conversation flow.

<br>

Output Key used the default: `response` and Input Key used the default: `input`.

<br>

In the LangFlow example, we used `ChatOpenAI` as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, [ChatOpenAI](https://platform.openai.com/){.internal-link target=\_blank} requires you to create an account to get your API key.

<br>

Check out the [ChatOpenAI](https://platform.openai.com/docs/introduction/overview){.internal-link target=\_blank} documentation to learn more about the API and the options that contain in the node.
